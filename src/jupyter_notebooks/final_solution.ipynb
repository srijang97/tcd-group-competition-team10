{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as ss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "data_dir = \"\"\n",
    "for dirname, _, filenames in os.walk('../../data/'):\n",
    "    data_dir = dirname\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = pd.read_csv(data_dir+\"/tcdml1920-rec-click-pred--training.csv\")\n",
    "raw_test_df = pd.read_csv(data_dir+\"/tcdml1920-rec-click-pred--test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning: Data split and column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_cleaning(train_df, test_df, split_method='time_viewed'):\n",
    "    \n",
    "    time_cols = ['query_word_count', 'query_detected_language', 'query_document_id', 'item_type', 'application_type', \n",
    "            'abstract_char_count', 'abstract_detected_language', \n",
    "            'app_lang', 'country_by_ip', 'local_hour_of_request', 'hour_request_received',\n",
    "            'algorithm_class','recommendation_algorithm_id_used', 'cbf_parser', 'search_title', 'search_keywords',\n",
    "            'search_abstract']    \n",
    "    \n",
    "   \n",
    "    jabref_cols = ['query_char_count', 'app_lang', 'app_version', 'country_by_ip', 'timezone_by_ip', \n",
    "               'local_hour_of_request', 'algorithm_class', 'recommendation_algorithm_id_used']\n",
    "\n",
    "    cbf_cols = ['cbf_parser']\n",
    "\n",
    "    com_blog_cols = ['query_word_count', 'query_detected_language', 'abstract_char_count',\n",
    "                     'application_type', 'hour_request_received', 'item_type',\n",
    "                     'country_by_ip', 'algorithm_class', 'recommendation_algorithm_id_used']\n",
    "    \n",
    "    id_col = ['recommendation_set_id']\n",
    "    \n",
    "    dep_cols = ['clicks', 'ctr', 'set_clicked']\n",
    "    \n",
    "    train_df = train_df[train_df['rec_processing_time']<40]\n",
    "    train_df = train_df[train_df['clicks']<= train_df['number_of_recs_in_set']]\n",
    "    \n",
    "    train_df = train_df[~(train_df['app_version'].isna())]\n",
    "    \n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for key, df in {'test':test_df, 'train':train_df}.items():\n",
    "        print(key)\n",
    "        df = df.replace(\"\\\\N\", np.nan)\n",
    "        cols_to_change = ['query_word_count', 'query_char_count', 'local_hour_of_request', 'recommendation_algorithm_id_used',\n",
    "                     'abstract_char_count', 'abstract_word_count']\n",
    "        df[cols_to_change] = df[cols_to_change].astype('float64')\n",
    "        df['q_doc_id_provided'] = df['query_document_id'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "        \n",
    "        data_dict = {}\n",
    "        if split_method=='app-cbf':\n",
    "            \n",
    "            if key=='test':\n",
    "                com_blog_cols = com_blog_cols + id_col\n",
    "                jabref_cols = jabref_cols + id_col\n",
    "            else:\n",
    "                com_blog_cols = list(set(com_blog_cols + ['set_clicked']) - set(id_col))\n",
    "                jabref_cols = list(set(jabref_cols + ['set_clicked']) - set(id_col))\n",
    "                \n",
    "            cblog_all = df[(df['application_type']=='blog') | (df['application_type']=='e-commerce') | (df['application_type']=='0')][com_blog_cols+cbf_cols]\n",
    "        \n",
    "            cblog_cbf = cblog_all[cblog_all['algorithm_class']=='content_based_filtering']\n",
    "            #cblog_cbf = cblog_cbf[~(cblog_cbf['country_by_ip'].isna())]\n",
    "            cblog_other = df[(df['application_type']=='blog') | (df['application_type']=='e-commerce') | (df['application_type']=='0')][com_blog_cols]\n",
    "            cblog_other = cblog_other[cblog_other['algorithm_class']!='content_based_filtering']\n",
    "#             cblog_other = cblog_other[~(cblog_other['country_by_ip'].isna())]\n",
    "            \n",
    "            dig_all = df[df['application_type']=='digital_library']\n",
    "            dig_cbf = dig_all[dig_all['algorithm_class']=='content_based_filtering'][jabref_cols+cbf_cols+ ['query_detected_language', 'q_doc_id_provided']]\n",
    "            #dig_cbf = dig_cbf[~((dig_cbf['country_by_ip'].isna()) | (dig_cbf['app_lang'].isna()) | (dig_cbf['app_version'].isna()))]\n",
    "            dig_other = df[df['application_type']=='digital_library'][jabref_cols]\n",
    "            dig_other = dig_other[dig_other['algorithm_class']!='content_based_filtering']\n",
    "            #dig_other = dig_other[~((dig_other['app_lang'].isna()) | (dig_other['app_version'].isna()))]\n",
    "            data_dict['cblog_cbf'] = cblog_cbf\n",
    "            data_dict['cblog_other'] = cblog_other\n",
    "            data_dict['dig_cbf'] = dig_cbf\n",
    "            data_dict['dig_other'] = dig_other\n",
    "            cleaned_data[key] = data_dict\n",
    "            \n",
    "        elif split_method=='time_viewed':\n",
    "            if key=='test':\n",
    "                df = df[~(df['application_type'].isna())]\n",
    "                cleaned_data[key] = df\n",
    "            else:\n",
    "                df = df[(~df['time_recs_viewed'].isna()) | (df['set_clicked']==1)]\n",
    "                df = df[time_cols + ['set_clicked']]\n",
    "                cleaned_data[key] = df\n",
    "        elif split_method=='algo':\n",
    "            pass        \n",
    "        \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['item_type', 'application_type', 'app_lang', 'app_version', 'country_by_ip', 'timezone_by_ip', \n",
    "            'algorithm_class','recommendation_algorithm_id_used',\n",
    "            'cbf_parser', 'query_detected_language', 'abstract_detected_language']\n",
    "\n",
    "cleaned_data = data_cleaning(raw_train_df, raw_test_df, 'time_viewed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get column-wise modes for missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_cols(df, group, grouped):\n",
    "    group_df = df.groupby(group).agg({grouped: lambda x: x.value_counts().index[0] if len(x.value_counts().index) > 0 else 'unknown'}).rename(columns={'<lambda>': grouped}).reset_index().set_index(group)\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing values for all datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset, data in cleaned_data.items():\n",
    "    for key, df in data.items():\n",
    "        \n",
    "        if key=='cblog_viewed':\n",
    "            group_df = group_cols(cleaned_data[dataset][key], ['organization_id'], 'item_type')\n",
    "            cleaned_data[dataset][key]['item_type'] = cleaned_data[dataset][key].apply(lambda x: group_df.loc[x['organization_id']][0] if pd.isnull(x['item_type']) else x['item_type'], axis=1)\n",
    "    \n",
    "            group_df = group_cols(cleaned_data[dataset][key], ['organization_id', 'hour_request_received'], 'country_by_ip')\n",
    "            cleaned_data[dataset][key]['country_by_ip'] = cleaned_data[dataset][key].apply(lambda x: group_df.loc[(x['organization_id'], x['hour_request_received'])][0] if pd.isnull(x['country_by_ip']) else x['country_by_ip'], axis=1)\n",
    "            \n",
    "            group_df = group_cols(cleaned_data[dataset][key], ['organization_id'], 'cbf_parser')\n",
    "            cleaned_data[dataset][key]['cbf_parser'] = cleaned_data[dataset][key].apply(lambda x: group_df.loc[x['organization_id']][0] if pd.isnull(x['cbf_parser']) else x['cbf_parser'], axis=1)\n",
    "            \n",
    "            missing_values = {'query_detected_language': df['query_detected_language'].mode()[0], \n",
    "#                               'abstract_char_count': df['abstract_char_count'].median(), \n",
    "                              'query_word_count': df['query_word_count'].median(),\n",
    "                             'app_lang': df['app_lang'].mode()[0], 'local_hour_of_request': df['local_hour_of_request'].median(),\n",
    "                     'abstract_char_count': df['abstract_char_count'].median(), 'abstract_detected_language': df['abstract_detected_language'].mode()[0]}\n",
    "            \n",
    "            cleaned_data[dataset][key] = df.fillna(value=missing_values)\n",
    "                    \n",
    "        elif key=='dig_cbf':\n",
    "            \n",
    "            missing_values = {'app_lang': df['app_lang'].mode()[0], 'app_version': df['app_version'].mode()[0], 'country_by_ip': 'missing', 'timezone_by_ip': 'missing', 'query_detected_language': df['query_detected_language'].mode()[0], \n",
    "                         'local_hour_of_request': df['local_hour_of_request'].median()}\n",
    "            cleaned_data[dataset][key]= df.fillna(value=missing_values)\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            missing_values = {'app_lang': df['app_lang'].mode()[0], 'app_version': df['app_version'].mode()[0], 'country_by_ip': 'missing', 'timezone_by_ip': 'missing', \n",
    "                'local_hour_of_request': df['local_hour_of_request'].median(), 'recommendation_algorithm_id_used': 33}\n",
    "            cleaned_data[dataset][key]= df.fillna(value=missing_values)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate weighted target encoding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_smooth_mean(df, df_train, by, on, m):\n",
    "    # Compute the global mean\n",
    "    mean = df_train[on].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df_train.groupby(by)[on].agg(['count', 'mean'])\n",
    "    \n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "    \n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "#     print(smooth.index)\n",
    "    # Replace each value by the according smoothed mean\n",
    "    return df[by].apply(lambda x: smooth.loc[str(x)] if str(x) in smooth.index else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(cleaned_data, cat_cols, target, scaling_method):\n",
    "    \n",
    "    processed_data = {}\n",
    "        \n",
    "    for dataset, data in cleaned_data.items():\n",
    "        processed_data[dataset] =  {}\n",
    "           \n",
    "        for key, df in data.items():\n",
    "            \n",
    "            \n",
    "            print('Type: ', dataset, ' Dataset: ', key, ' Shape: ', df.shape)\n",
    "            df_cat_cols = list(set(cat_cols).intersection(set(df.columns)))\n",
    "            \n",
    "            if dataset=='train':\n",
    "                df_num_cols = list(set(df.columns) - set(df_cat_cols) -set([target]))\n",
    "            else:\n",
    "                df_num_cols = list(set(df.columns) - set(df_cat_cols) - set(['recommendation_set_id']))\n",
    "                \n",
    "            if scaling_method=='standard':\n",
    "                scaler = StandardScaler()\n",
    "                df[df_num_cols] = scaler.fit_transform(df[df_num_cols])\n",
    "                \n",
    "            else:\n",
    "                scaler = MinMaxScaler()\n",
    "                df[df_num_cols] = scaler.fit_transform(df[df_num_cols])\n",
    "            \n",
    "            for col in df_cat_cols:\n",
    "#                 df_x = cleaned_data['train'][key][[col, 'set_clicked']].astype({col: str}).groupby(by=[col]).mean()           \n",
    "#                 df[col] = df[col].apply(lambda x: df_x.loc[str(x)][0] if str(x) in df_x.index else 0) \n",
    "                df[col] = calc_smooth_mean(df, cleaned_data['train'][key], col, 'set_clicked', 150)\n",
    "            \n",
    "        \n",
    "            if dataset=='train': \n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(df[df_num_cols+df_cat_cols].values, df[target].values, test_size=0.3)\n",
    "                processed_data[dataset][key] = {'X': df[df_num_cols+df_cat_cols].values, 'y': df[target].values,\n",
    "                                                'X_train': X_train, 'X_valid': X_valid, 'y_train': y_train, 'y_valid': y_valid}\n",
    "            else:\n",
    "                processed_data[dataset][key] = {'data': df[df_num_cols + df_cat_cols].values, 'id': df['recommendation_set_id'].values}\n",
    "                \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_data = preprocess_data(cleaned_data, cat_cols, 'set_clicked', 'standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Application and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = {}\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "for key, data in processed_data['test'].items():\n",
    "    X = processed_data['train'][key]['X']\n",
    "    y = processed_data['train'][key]['y']\n",
    "    \n",
    "    df = pd.concat([pd.DataFrame(X), pd.Series(y)], axis=1)\n",
    "    df_0 = df[df.iloc[:, -1]==0]\n",
    "    df_1 = df[df.iloc[:, -1]==1]\n",
    "    \n",
    "    #Downsample majority class data\n",
    "    df_0_new = resample(df_0, replace=True, n_samples=df_1.shape[0]*12)\n",
    "    \n",
    "    df = pd.concat([df_0_new, df_1])\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[: , -1].values\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    y_pred= model.predict(data['data'])\n",
    "    test_preds[key] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating the Results and saving to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.vstack((np.hstack((processed_data['test']['cblog_viewed']['id'].reshape(-1, 1), test_preds['cblog_viewed'].reshape(-1, 1))),\n",
    "                   np.hstack((processed_data['test']['dig_cbf']['id'].reshape(-1, 1), test_preds['dig_cbf'].reshape(-1, 1))),\n",
    "                   np.hstack((processed_data['test']['dig_other']['id'].reshape(-1, 1), test_preds['dig_other'].reshape(-1, 1)))))\n",
    "    \n",
    "result = pd.DataFrame(data = result, columns=['recommendation_set_id', 'set_clicked'])\n",
    "result['recommendation_set_id'] = result['recommendation_set_id'].astype(int)\n",
    "result['set_clicked'] = result['set_clicked'].astype(int)\n",
    "result.sort_values(by='recommendation_set_id', inplace=True)\n",
    "result.to_csv('Submission-file.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
