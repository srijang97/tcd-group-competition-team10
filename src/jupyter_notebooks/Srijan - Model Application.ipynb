{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/tcdml1920-rec-click-pred--submission file (example).csv\n",
      "../../data/tcdml1920-rec-click-pred--submission file.csv\n",
      "../../data/tcdml1920-rec-click-pred--test.csv\n",
      "../../data/tcdml1920-rec-click-pred--training.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as ss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "data_dir = \"\"\n",
    "for dirname, _, filenames in os.walk('../../data/'):\n",
    "    data_dir = dirname\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (4,5,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "raw_train_df = pd.read_csv(data_dir+\"/tcdml1920-rec-click-pred--training.csv\")\n",
    "raw_test_df = pd.read_csv(data_dir+\"/tcdml1920-rec-click-pred--test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(train_df, test_df, split_method='time_viewed'):\n",
    "    \n",
    "    time_cols = ['query_word_count', 'query_detected_language', 'query_document_id', 'item_type', 'application_type', \n",
    "            'abstract_char_count', 'abstract_detected_language', \n",
    "            'app_lang', 'country_by_ip', 'local_hour_of_request', 'hour_request_received',\n",
    "            'algorithm_class','recommendation_algorithm_id_used', 'cbf_parser', 'search_title', 'search_keywords',\n",
    "            'search_abstract']    \n",
    "    \n",
    "   \n",
    "    jabref_cols = ['query_char_count', 'app_lang', 'app_version', 'country_by_ip', 'timezone_by_ip', \n",
    "               'local_hour_of_request', 'algorithm_class', 'recommendation_algorithm_id_used']\n",
    "\n",
    "    cbf_cols = ['cbf_parser']\n",
    "\n",
    "    com_blog_cols = ['query_word_count', 'query_detected_language', 'abstract_char_count',\n",
    "                     'application_type', 'hour_request_received', 'item_type',\n",
    "                     'country_by_ip', 'algorithm_class', 'recommendation_algorithm_id_used']\n",
    "    \n",
    "    id_col = ['recommendation_set_id']\n",
    "    \n",
    "    dep_cols = ['clicks', 'ctr', 'set_clicked']\n",
    "    \n",
    "    train_df = train_df[train_df['rec_processing_time']<40]\n",
    "    train_df = train_df[train_df['clicks']<= train_df['number_of_recs_in_set']]\n",
    "    \n",
    "    train_df = train_df[~(train_df['app_version'].isna())]\n",
    "    \n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for key, df in {'test':test_df, 'train':train_df}.items():\n",
    "        print(key)\n",
    "        df = df.replace(\"\\\\N\", np.nan)\n",
    "        cols_to_change = ['query_word_count', 'query_char_count', 'local_hour_of_request', 'recommendation_algorithm_id_used',\n",
    "                     'abstract_char_count', 'abstract_word_count']\n",
    "        df[cols_to_change] = df[cols_to_change].astype('float64')\n",
    "        df['q_doc_id_provided'] = df['query_document_id'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "        \n",
    "        data_dict = {}\n",
    "        if split_method=='app-cbf':\n",
    "            \n",
    "            if key=='test':\n",
    "                com_blog_cols = com_blog_cols + id_col\n",
    "                jabref_cols = jabref_cols + id_col\n",
    "            else:\n",
    "                com_blog_cols = list(set(com_blog_cols + ['set_clicked']) - set(id_col))\n",
    "                jabref_cols = list(set(jabref_cols + ['set_clicked']) - set(id_col))\n",
    "                \n",
    "            cblog_all = df[(df['application_type']=='blog') | (df['application_type']=='e-commerce') | (df['application_type']=='0')][com_blog_cols+cbf_cols]\n",
    "        \n",
    "            cblog_cbf = cblog_all[cblog_all['algorithm_class']=='content_based_filtering']\n",
    "            #cblog_cbf = cblog_cbf[~(cblog_cbf['country_by_ip'].isna())]\n",
    "            cblog_other = df[(df['application_type']=='blog') | (df['application_type']=='e-commerce') | (df['application_type']=='0')][com_blog_cols]\n",
    "            cblog_other = cblog_other[cblog_other['algorithm_class']!='content_based_filtering']\n",
    "#             cblog_other = cblog_other[~(cblog_other['country_by_ip'].isna())]\n",
    "            \n",
    "            dig_all = df[df['application_type']=='digital_library']\n",
    "            dig_cbf = dig_all[dig_all['algorithm_class']=='content_based_filtering'][jabref_cols+cbf_cols+ ['query_detected_language', 'q_doc_id_provided']]\n",
    "            #dig_cbf = dig_cbf[~((dig_cbf['country_by_ip'].isna()) | (dig_cbf['app_lang'].isna()) | (dig_cbf['app_version'].isna()))]\n",
    "            dig_other = df[df['application_type']=='digital_library'][jabref_cols]\n",
    "            dig_other = dig_other[dig_other['algorithm_class']!='content_based_filtering']\n",
    "            #dig_other = dig_other[~((dig_other['app_lang'].isna()) | (dig_other['app_version'].isna()))]\n",
    "            data_dict['cblog_cbf'] = cblog_cbf\n",
    "            data_dict['cblog_other'] = cblog_other\n",
    "            data_dict['dig_cbf'] = dig_cbf\n",
    "            data_dict['dig_other'] = dig_other\n",
    "            cleaned_data[key] = data_dict\n",
    "            \n",
    "        elif split_method=='time_viewed':\n",
    "            if key=='test':\n",
    "                df = df[~(df['application_type'].isna())]\n",
    "                cleaned_data[key] = df\n",
    "            else:\n",
    "                df = df[(~df['time_recs_viewed'].isna()) | (df['set_clicked']==1)]\n",
    "                df = df[time_cols + ['set_clicked']]\n",
    "                cleaned_data[key] = df\n",
    "        elif split_method=='algo':\n",
    "            pass        \n",
    "        \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['item_type', 'application_type', 'app_lang', 'app_version', 'country_by_ip', 'timezone_by_ip', \n",
    "            'algorithm_class','recommendation_algorithm_id_used',\n",
    "            'cbf_parser', 'query_detected_language', 'abstract_detected_language']\n",
    "\n",
    "cleaned_data = data_cleaning(raw_train_df, raw_test_df, 'time_viewed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9145\n"
     ]
    }
   ],
   "source": [
    "testlen = 0\n",
    "for key, data in cleaned_data['test'].items():\n",
    "    testlen = testlen + data.shape[0]\n",
    "\n",
    "print(testlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DE    2653\n",
       "US    2207\n",
       "CN     353\n",
       "BR     283\n",
       "GB     275\n",
       "FR     203\n",
       "CA     197\n",
       "AT     188\n",
       "AU     183\n",
       "IN     162\n",
       "IT     159\n",
       "IE     157\n",
       "CH     145\n",
       "ES     135\n",
       "NL     127\n",
       "RU     100\n",
       "JP      96\n",
       "BE      93\n",
       "MX      85\n",
       "PL      84\n",
       "CO      64\n",
       "PT      56\n",
       "DK      54\n",
       "\\N      51\n",
       "NO      51\n",
       "ID      51\n",
       "SE      49\n",
       "PK      42\n",
       "ZA      42\n",
       "TR      38\n",
       "EC      35\n",
       "GR      34\n",
       "CZ      34\n",
       "SG      32\n",
       "NZ      31\n",
       "FI      29\n",
       "KR      29\n",
       "TW      28\n",
       "HK      27\n",
       "PH      22\n",
       "CL      21\n",
       "DZ      21\n",
       "MY      19\n",
       "TH      19\n",
       "PE      19\n",
       "IL      19\n",
       "AR      18\n",
       "HU      17\n",
       "RO      15\n",
       "IR      14\n",
       "KE      14\n",
       "VN      14\n",
       "LT      12\n",
       "SA      12\n",
       "RS      12\n",
       "EE      11\n",
       "EG      10\n",
       "LV      10\n",
       "HR       9\n",
       "TN       8\n",
       "LU       8\n",
       "BD       7\n",
       "UA       7\n",
       "GH       7\n",
       "SK       7\n",
       "CR       7\n",
       "TT       6\n",
       "SI       6\n",
       "BG       6\n",
       "AE       6\n",
       "BA       6\n",
       "NG       5\n",
       "UY       5\n",
       "BO       4\n",
       "MU       4\n",
       "LK       4\n",
       "BY       4\n",
       "MA       4\n",
       "BJ       3\n",
       "IS       3\n",
       "PR       3\n",
       "PY       3\n",
       "PS       2\n",
       "TZ       2\n",
       "NP       2\n",
       "UG       2\n",
       "KZ       2\n",
       "CM       2\n",
       "ET       2\n",
       "BH       2\n",
       "GT       2\n",
       "IQ       2\n",
       "SO       2\n",
       "VE       2\n",
       "JO       2\n",
       "SR       1\n",
       "LY       1\n",
       "CU       1\n",
       "MT       1\n",
       "ZM       1\n",
       "MD       1\n",
       "LB       1\n",
       "DO       1\n",
       "MK       1\n",
       "KW       1\n",
       "BW       1\n",
       "AL       1\n",
       "HN       1\n",
       "RE       1\n",
       "NE       1\n",
       "GU       1\n",
       "GE       1\n",
       "KH       1\n",
       "GL       1\n",
       "OM       1\n",
       "SL       1\n",
       "SD       1\n",
       "QA       1\n",
       "ML       1\n",
       "Name: country_by_ip, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_df[raw_test_df['country_by_ip'].isin(country_index)]['country_by_ip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset:  cblog_cbf\n",
      "\n",
      "Dataset:  cblog_other\n",
      "\n",
      "Dataset:  dig_cbf\n",
      "\n",
      "Dataset:  dig_other\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in cleaned_data['train'].items():\n",
    "    print('\\nDataset: ', key)\n",
    "    for col in dataset.columns:\n",
    "        if np.nan in dataset[col].value_counts(dropna=False).index:\n",
    "            print('Col: ', col)\n",
    "            num = dataset[col].value_counts(dropna=False).loc[np.nan]\n",
    "            percent = dataset[col].value_counts(dropna=False, normalize= True).loc[np.nan]\n",
    "            print('Num: ', num, ' Percent: ', percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col:  query_word_count\n",
      "Num:  7  Percent:  0.0003633532312483779\n",
      "Col:  query_detected_language\n",
      "Num:  37  Percent:  0.0019205813651699973\n",
      "Col:  query_document_id\n",
      "Num:  4112  Percent:  0.21344406955618997\n",
      "Col:  item_type\n",
      "Num:  2116  Percent:  0.10983649104593823\n",
      "Col:  abstract_char_count\n",
      "Num:  4584  Percent:  0.23794445886322346\n",
      "Col:  abstract_detected_language\n",
      "Num:  4585  Percent:  0.23799636646768751\n",
      "Col:  app_lang\n",
      "Num:  4477  Percent:  0.2323903451855697\n",
      "Col:  country_by_ip\n",
      "Num:  142  Percent:  0.0073708798338956655\n",
      "Col:  local_hour_of_request\n",
      "Num:  1787  Percent:  0.09275888917726446\n",
      "Col:  recommendation_algorithm_id_used\n",
      "Num:  277  Percent:  0.014378406436542954\n",
      "Col:  cbf_parser\n",
      "Num:  6230  Percent:  0.32338437581105633\n"
     ]
    }
   ],
   "source": [
    "for col in cleaned_data['train'].columns:\n",
    "        if np.nan in cleaned_data['train'][col].value_counts(dropna=False).index:\n",
    "            print('Col: ', col)\n",
    "            num = cleaned_data['train'][col].value_counts(dropna=False).loc[np.nan]\n",
    "            percent = cleaned_data['train'][col].value_counts(dropna=False, normalize= True).loc[np.nan]\n",
    "            print('Num: ', num, ' Percent: ', percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, data in cleaned_data.items():\n",
    "    for key, df in data.items():\n",
    "        \n",
    "        if key=='cblog_cbf':\n",
    "            missing_values = {'query_detected_language': df['query_detected_language'].mode()[0], \n",
    "                              'country_by_ip': 'missing', 'abstract_char_count': df['abstract_char_count'].median(), \n",
    "                            'item_type': 'missing', 'query_word_count': df['query_word_count'].median()}\n",
    "            cleaned_data[dataset][key] = df.fillna(value=missing_values)\n",
    "            \n",
    "        elif key=='cblog_other':    \n",
    "            missing_values = {'query_detected_language': df['query_detected_language'].mode()[0],\n",
    "                              'abstract_char_count': df['abstract_char_count'].median(), 'query_word_count': df['query_word_count'].median(),\n",
    "                         'item_type': 'missing', 'country_by_ip': 'missing'}\n",
    "            cleaned_data[dataset][key] = df.fillna(value=missing_values)\n",
    "           \n",
    "        elif key=='dig_cbf':\n",
    "            missing_values = {'app_lang': df['app_lang'].mode()[0], 'country_by_ip': 'missing', 'timezone_by_ip': 'missing', 'query_detected_language': df['query_detected_language'].mode()[0], \n",
    "                         'local_hour_of_request': df['local_hour_of_request'].median()}\n",
    "            cleaned_data[dataset][key]= df.fillna(value=missing_values)\n",
    "            \n",
    "        else:\n",
    "            missing_values = {'app_lang': df['app_lang'].mode()[0], 'country_by_ip': 'missing', 'timezone_by_ip': 'missing', \n",
    "                'local_hour_of_request': df['local_hour_of_request'].median(), 'recommendation_algorithm_id_used': 33}\n",
    "            cleaned_data[dataset][key]= df.fillna(value=missing_values)\n",
    "            \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(cleaned_data, cat_cols, target, scaling_method):\n",
    "    \n",
    "    processed_data = {}\n",
    "    \n",
    "#     for key, df in cleaned_data['train']:\n",
    "#         train_df = cleaned_data['train'][key]\n",
    "#         test_df = cleaned_data['test'][key]\n",
    "        \n",
    "        \n",
    "    for dataset, data in cleaned_data.items():\n",
    "        processed_data[dataset] = {}\n",
    "        for key, df in data.items():\n",
    "            \n",
    "            print('Type: ', dataset, ' Dataset: ', key, ' Shape: ', df.shape)\n",
    "            df_cat_cols = list(set(cat_cols).intersection(set(df.columns)))\n",
    "            \n",
    "            if dataset=='train':\n",
    "                df_num_cols = list(set(df.columns) - set(df_cat_cols) -set([target]))\n",
    "            else:\n",
    "                df_num_cols = list(set(df.columns) - set(df_cat_cols) - set(['recommendation_set_id']))\n",
    "                \n",
    "            if scaling_method=='standard':\n",
    "                scaler = StandardScaler()\n",
    "                df[df_num_cols] = scaler.fit_transform(df[df_num_cols])\n",
    "                \n",
    "            else:\n",
    "                scaler = MinMaxScaler()\n",
    "                df[df_num_cols] = scaler.fit_transform(df[df_num_cols])\n",
    "            \n",
    "            for col in df_cat_cols:\n",
    "                df_x = cleaned_data['train'][key][[col, 'set_clicked']].astype({col: str}).groupby(by=[col]).mean()           \n",
    "                df[col] = df[col].apply(lambda x: df_x.loc[str(x)][0] if str(x) in df_x.index else 0)   \n",
    "            \n",
    "        \n",
    "            if dataset=='train': \n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(df[df_num_cols+df_cat_cols].values, df[target].values, test_size=0.3)\n",
    "                processed_data[dataset][key] = {'X': df[df_num_cols+df_cat_cols].values, 'y': df[target].values,\n",
    "                                                'X_train': X_train, 'X_valid': X_valid, 'y_train': y_train, 'y_valid': y_valid}\n",
    "            else:\n",
    "                processed_data[dataset][key] = {'data': df[df_num_cols + df_cat_cols], 'id': df['recommendation_set_id']}\n",
    "                \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  test  Dataset:  cblog_cbf  Shape:  (2663, 11)\n",
      "Type:  test  Dataset:  cblog_other  Shape:  (1308, 10)\n",
      "Type:  test  Dataset:  dig_cbf  Shape:  (3795, 12)\n",
      "Type:  test  Dataset:  dig_other  Shape:  (1379, 9)\n",
      "Type:  train  Dataset:  cblog_cbf  Shape:  (81623, 11)\n",
      "Type:  train  Dataset:  cblog_other  Shape:  (33909, 10)\n",
      "Type:  train  Dataset:  dig_cbf  Shape:  (193290, 12)\n",
      "Type:  train  Dataset:  dig_other  Shape:  (76671, 9)\n"
     ]
    }
   ],
   "source": [
    "processed_data = preprocess_data(cleaned_data, cat_cols, 'set_clicked', 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33830, 9)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['train']['cblog_other']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "for key, data in processed_data['train'].items():\n",
    "    logit = LogisticRegression()\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(data['X'], data['y'], test_size=0.2)\n",
    "    logit.fit(X_train, y_train)\n",
    "    y_pred= logit.predict(X_valid)\n",
    "    \n",
    "    f1 = f1_score(y_valid, y_pred, average='micro')\n",
    "    precision = precision_score(y_valid, y_pred, average='micro')\n",
    "    recall = recall_score(y_valid, y_pred, average='micro')\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    results[key] = {'F1':f1, ' Precision': precision, 'Recall': recall, 'CM': cm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = {}\n",
    "#logit = LogisticRegression()\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "for key, data in processed_data['test'].items():\n",
    "    \n",
    "    model.fit(processed_data['train'][key]['X'], processed_data['train'][key]['y'])\n",
    "    y_pred= model.predict(data['data'])\n",
    "    test_preds[key] = y_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.vstack((np.hstack((processed_data['test']['cblog_cbf']['id'].values.reshape(-1, 1), test_preds['cblog_cbf'].reshape(-1, 1))),\n",
    "                   np.hstack((processed_data['test']['cblog_other']['id'].values.reshape(-1, 1), test_preds['cblog_other'].reshape(-1, 1))),\n",
    "                   np.hstack((processed_data['test']['dig_cbf']['id'].values.reshape(-1, 1), test_preds['dig_cbf'].reshape(-1, 1))),\n",
    "                   np.hstack((processed_data['test']['dig_other']['id'].values.reshape(-1, 1), test_preds['dig_other'].reshape(-1, 1)))))\n",
    "    \n",
    "result = pd.DataFrame(data = result, columns=['recommendation_set_id', 'set_clicked'])\n",
    "result['recommendation_set_id'] = result['recommendation_set_id'].astype(int)\n",
    "result.to_csv('tcdml1920-rec-click-pred--submission file.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['set_clicked'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
